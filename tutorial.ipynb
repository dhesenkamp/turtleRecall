{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbajaeYF-e59"
      },
      "source": [
        "# Turtle Recall Tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4e72p3aSf_q"
      },
      "source": [
        "Welcome to the tutorial for the **Turtle Recall: Conservation Challenge [Zindi competition](https://zindi.africa/competitions/turtle-recall-conservation-challenge)**!\n",
        "\n",
        "In this notebook, we'll be taking you through an explanation of the challenge, dataset, and an example approach for building a model that can recognise faces of individual turtles.\n",
        "\n",
        "\u003cp align=\"center\"\u003e\n",
        "  \u003cimg src=\"https://storage.googleapis.com/dm-turtle-recall/tutorial_images/shelfie.jpg\" width=\"512\"/\u003e\n",
        "\u003c/p\u003e\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZElN4W_U22Q"
      },
      "source": [
        "Table of contents:\n",
        "\n",
        "1. [Introduction](#Introduction)\n",
        "2. [Dataset overview](#Dataset)\n",
        "3. [Data exploration](#Exploration)\n",
        "4. [Approaching the modelling problem](#Approach)\n",
        "5. [A quick introduction to JAX and Haiku](#JAX)\n",
        "6. [Fine-tuning a ResNet model using JAX and Haiku](#Model)\n",
        "7. [Submitting our predictions](#Submit)\n",
        "8. [Suggestions for improving the model](#Suggestions)\n",
        "9. [License and Disclaimer](#Legal)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNMvC2WNS0q-"
      },
      "source": [
        "\u003ca name=\"Introduction\"\u003e\u003c/a\u003e\n",
        "## 1. Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQqBJd6nS4KV"
      },
      "source": [
        "The aim of this competition is to build a machine learning model to identify individual sea turtles. For each image presented, the model should output the turtle's unique ID or, if the image corresponds to a new turtle (not present in the database), identify it as a new individual."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8hy8iWcS4Md"
      },
      "source": [
        "\u003ca name=\"Dataset\"\u003e\u003c/a\u003e\n",
        "## 2. Dataset overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-FXmdJhS0tM"
      },
      "source": [
        "The dataset provided with this challenge consists of the following files:\n",
        "- `train.csv` containing 3 columns:\n",
        "  - `image_id`: image identifier, mapping to `.JPG` image files located in the `IMAGES` subfolder.\n",
        "  - `image_location`: this indicates whether the image is the left, right or top view of the turtle's face.\n",
        "  - `turtle_id`: unique turtle identifier (what your model will be trained to predict given the image)\n",
        "- `extra_images.csv` of turtle images containing 2 columns, `image_id` and `turtle_id`.\n",
        "- `test.csv` contains test set image annotation with two columns `image_id` and `image_location`\n",
        "  - Note that one possible outcome class is `new_turtle`, meaning that you may be asked to classify turtles that were not previously observed in the training set.\n",
        "- `sample_submission.csv`: example submission file showing the 2 column format for submissions:\n",
        "  - The first column is an `image_id`\n",
        "  - The second column is a list of 5 `turtle_id` entries showing your top 5 predictions for the identity of the turtle shown in the image.\n",
        "\n",
        "To get a sense of the data, here are a few examples from the training dataset:\n",
        "\n",
        "\u003cp align=\"center\"\u003e\n",
        "  \u003cimg src=\"https://storage.googleapis.com/dm-turtle-recall/tutorial_images/training_examples.png\" width=\"600\"/\u003e\n",
        "\u003c/p\u003e\n",
        "\n",
        "You can maybe already see some of the potential challenges inherent in this dataset!\n",
        "- **Face positioning**. Turtle face pictures can be taken from 3 main angles, and even within a position category like \"left\", head position can vary.\n",
        "- **Image dimensions**. Image dimensions and aspect ratios can vary, e.g. the \"top\" image is in tall format whereas the other three are in wide format.\n",
        "- **Image quality**. The blurriness vs. sharpness of the images can vary\n",
        "\n",
        "Looking through other images, you'll find other sources of diversity:\n",
        "- Differences in backgrounds and lighting conditions\n",
        "- Extraneous objects in the image such as hands, people in the background, and tags (not related to `turtle_id`)\n",
        "- Turtles can have varying amounts of sand on their head\n",
        "\n",
        "Here is a collage of some raw, unreshaped image samples from the training dataset:\n",
        "\n",
        "\u003cp align=\"center\"\u003e\n",
        "  \u003cimg src=\"https://storage.googleapis.com/dm-turtle-recall/tutorial_images/training_image_collage.png\" width=\"800\"/\u003e\n",
        "\u003c/p\u003e\n",
        "\n",
        "The challenge is to be able to recognise turtle faces despite the noise and confounding factors in the images!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80wasOIL8kya"
      },
      "source": [
        "\u003ca name=\"Exploration\"\u003e\u003c/a\u003e\n",
        "## 3. Data exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnYW2dnSVZz6"
      },
      "source": [
        "Next, we can load the train and test datasets and start to make some exploratory plots.\n",
        "\n",
        "\n",
        "\n",
        "We download the images from a Google Cloud storage bucket (could take some time):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLsn83pVMqDw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "SOURCE_URL = 'https://storage.googleapis.com/dm-turtle-recall/images.tar'\n",
        "IMAGE_DIR = './turtle_recall/images'\n",
        "TAR_PATH = os.path.join(IMAGE_DIR, os.path.basename(SOURCE_URL))\n",
        "EXPECTED_IMAGE_COUNT = 13891\n",
        "\n",
        "%sx mkdir --parents \"{IMAGE_DIR}\"\n",
        "if len(os.listdir(IMAGE_DIR)) != EXPECTED_IMAGE_COUNT:\n",
        "  %sx wget --no-check-certificate -O \"{TAR_PATH}\" \"{SOURCE_URL}\"\n",
        "  %sx tar --extract --file=\"{TAR_PATH}\" --directory=\"{IMAGE_DIR}\"\n",
        "  %sx rm \"{TAR_PATH}\"\n",
        "\n",
        "print(f'The total number of images is: {len(os.listdir(IMAGE_DIR))}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFo2l5BYegGv"
      },
      "source": [
        "Read in the train, test, and sample submission CSV files as pandas dataframes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_Q-Bqfn9Dry"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import io\n",
        "import urllib.parse\n",
        "\n",
        "BASE_URL = 'https://storage.googleapis.com/dm-turtle-recall/'\n",
        "\n",
        "\n",
        "def read_csv_from_web(file_name):\n",
        "  url = urllib.parse.urljoin(BASE_URL, file_name)\n",
        "  content = requests.get(url).content\n",
        "  return pd.read_csv(io.StringIO(content.decode('utf-8')))\n",
        "\n",
        "\n",
        "# Read in csv files.\n",
        "train = read_csv_from_web('train.csv')\n",
        "test = read_csv_from_web('test.csv')\n",
        "sample_submission = read_csv_from_web('sample_submission.csv')\n",
        "\n",
        "# Convert image_location strings to lowercase.\n",
        "for df in [train, test]:\n",
        "  df.image_location = df.image_location.apply(lambda x: x.lower())\n",
        "  assert set(df.image_location.unique()) == set(['left', 'right', 'top'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdvHdtUeGPau"
      },
      "source": [
        "Check out our dataframes and print their shapes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6DCF9uWGUOk"
      },
      "outputs": [],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52Q0kF8cGYfd"
      },
      "outputs": [],
      "source": [
        "test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDa9d7YQGaSl"
      },
      "outputs": [],
      "source": [
        "sample_submission.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAJFgifWIn_w"
      },
      "outputs": [],
      "source": [
        "train.shape, test.shape, sample_submission.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x63GkXaiAOE8"
      },
      "source": [
        "How many unique turtles are in the training set?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnTPq_zlG3VL"
      },
      "outputs": [],
      "source": [
        "print(f\"There are {train.turtle_id.nunique()} unique turtles in the train set.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mhAjSDRDhjb"
      },
      "source": [
        "How many images are there for each individual turtle in the training set?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Er5GYDiE5iB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_images_per_turtle = pd.value_counts(train['turtle_id'])\n",
        "print('The mean number of training images per turtle is '\n",
        "      f'{round(np.mean(train_images_per_turtle), 2)}, '\n",
        "      f'and the median is {int(np.median(train_images_per_turtle))}.')\n",
        "sns.histplot(train_images_per_turtle)\n",
        "plt.xlabel('Images per train turtle')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J39WSJGCG_Et"
      },
      "source": [
        "We can plot the number of images per `turtle_id`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmTyBy7mG9I-"
      },
      "outputs": [],
      "source": [
        "images_per_turtle = pd.value_counts(train['turtle_id'])\n",
        "plt.figure(figsize=(3, 21))\n",
        "sns.barplot(x=images_per_turtle, y=images_per_turtle.index,\n",
        "            palette='Blues_r', orient='horizontal')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K707qq-vFcJ8"
      },
      "source": [
        "Some observations from these plots:\n",
        "- This dataset is small, with not many images per class.\n",
        "- This dataset is characterised by class imbalance, with some turtles having many more training images available than others.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lbc3N6khS6qK"
      },
      "source": [
        "\u003ca name=\"Approach\"\u003e\u003c/a\u003e\n",
        "## 4. Approaching the modelling problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdS9--dVVGWB"
      },
      "source": [
        "Since we want to match images to labels (classes), we are dealing with an **image classification** problem. Image classification is generally most successfully approached using deep **convolutional neural networks** (CNNs).\n",
        "\n",
        "### 4.1 Convolutional neural networks\n",
        "\n",
        "At the highest level, CNNs take images as inputs and return probabilities that the image belongs to each of the possible classes.\n",
        "\n",
        "In slightly more detail, CNNs hierarchically extract features from images using convolutional layers, which are usually followed by pooling layers that summarise the information in the extracted feature maps:\n",
        "- Lower CNN layers capture low-level image features (edges, blobs)\n",
        "- Layers deeper in the CNN capture higher-level features and objects (scale patterns, turtle eyes)\n",
        "- Fully connected layers can then consolidate these extracted patterns and objects\n",
        "\n",
        "\u003cp align=\"center\"\u003e\n",
        "  \u003cimg src=\"https://storage.googleapis.com/dm-turtle-recall/tutorial_images/cnn.png\" width=\"1100\"/\u003e\n",
        "\u003c/p\u003e\n",
        "\n",
        "We won't go into a detailed explanation of CNNs here; if you'd like to learn more about their inner workings, we recommend [CS231n: Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io/).\n",
        "\n",
        "### 4.2 Transfer learning for image classification\n",
        "\n",
        "To approach this turtle face classification problem, we could specify a CNN architecture, initialise it, and proceed to train its parameters from scratch.\n",
        "\n",
        "An alternative approach, which is very popular in the computer vision (image processing) world, is called **transfer learning**:\n",
        "- Instead of training your model on your task from scratch, you first identify a model that has been **pre-trained** on some other image task\n",
        "- You can then adapt it (**\"fine-tune\"** it) to your specific task\n",
        "\n",
        "This is a popular approach because it often gets you two advantages:\n",
        "- **Improved performance**. The pre-training does a lot of the heavy lifting of learning to understand images. The most popular pre-trained models were trained on huge datasets for a long period of time, and you can simply piggy back on what they've learned.\n",
        "  - The features that these models learn are often reusable for many image-related tasks.\n",
        "  - This is because lower level visual features such as edges, colour blobs, simple composite shapes etc. are pervasive regardless of the specifics of the image task.\n",
        "- **Lower data and computational requirements**. Using a pre-trained model often means you don't need as much training data, time, or compute to reach a certain level of performance.\n",
        "\n",
        "There are plenty of pre-trained computer vision models available, the most popular probably being VGG, ResNet, EfficientNet, etc.\n",
        "\n",
        "Here, we will make use of a pre-trained ResNetX model made available via the **Haiku** library (a tool for building neural networks in **JAX**).\n",
        "\n",
        "\n",
        "\u003cp align=\"center\"\u003e\n",
        "  \u003cimg src=\"https://storage.googleapis.com/dm-turtle-recall/tutorial_images/jax.png\" width=\"900\"/\u003e\n",
        "\u003c/p\u003e\n",
        "\n",
        "The rest of this tutorial will implement an image classification neural network using JAX, which is a language or framework for numerical computation and deep learning that is frequently used by researchers and engineers at DeepMind. Since JAX is still less common than other tools such as Keras, Tensorflow, and PyTorch, the next section contains a quick introduction to some of the core features of JAX and Haiku.\n",
        "\n",
        "**Note**: It is not at all necessary to use JAX for this challenge, but we thought this might be a nice opportunity to introduce more people to this powerful and elegant way of writing machine learning code.\n",
        "\n",
        "If you are already familiar with JAX, feel free to skip the next section :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKf7bu9YTE1R"
      },
      "source": [
        "\u003ca name=\"JAX\"\u003e\u003c/a\u003e\n",
        "## 5. A very quick introduction to JAX and Haiku"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJr9srSq2dPL"
      },
      "source": [
        "JAX is an open-source Python library that facilitates high-performance machine learning. JAX is sometimes called **\"accelerated NumPy\"**, in that JAX code is very similar to NumPy code (with many direct equivalents to NumPy commands) but offers various speedups, efficiency improvements, and built-in GPU/TPU support compared to NumPy.\n",
        "\n",
        "### 5.1 JAX overview\n",
        "\n",
        "JAX is based on the following principles:\n",
        "- **Just-In-Time (JIT) compilation**. JAX can substantially speed up your code by using a special compiler called XLA (accelerated linear algebra) under the hood that fuses operations in your code, enabling it to run faster and more efficiently.\n",
        "- **Automatic differentiation**\n",
        "- **Vectorisation**\n",
        "- **Parallelism**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhEOwCM1GqIG"
      },
      "source": [
        "Here is a simple (non-ML) example of JAX usage:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tchI1qxeGokp"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "try:\n",
        "  import haiku as hk\n",
        "except ModuleNotFoundError:\n",
        "  !pip install dm-haiku\n",
        "  import haiku as hk\n",
        "\n",
        "# Define a function.\n",
        "def f(x):\n",
        "  return x ** 2\n",
        "\n",
        "# Wrap function in `jax.grad` to get the gradient.\n",
        "g = jax.vmap(jax.grad(f))\n",
        "\n",
        "# Visualize function and derivative for x in [-10, 10].\n",
        "x = jnp.arange(-10, 10, 0.01)\n",
        "y = f(x)\n",
        "dydx = g(x)\n",
        "plt.plot(x, y)\n",
        "plt.plot(x, dydx)\n",
        "plt.legend([\"y=f(x)=x^2\", \"dydx = g(x) = f'(x) = 2x\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfe5ZaK8GvrR"
      },
      "source": [
        "We can greatly speed up repeated calculations by using `jax.jit` for Just-In-Time compilation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBZLke3BGv98"
      },
      "outputs": [],
      "source": [
        "key = jax.random.PRNGKey(1)\n",
        "key, subkey1, subkey2 = jax.random.split(key, num=3)\n",
        "a = jax.random.uniform(subkey1, shape=(128, 1000, 1000))\n",
        "b = jax.random.uniform(subkey2, shape=(128, 1000, 256))\n",
        "\n",
        "# Define a function.\n",
        "def f(a, b):\n",
        "  a = jax.nn.softmax(a, axis=-1)\n",
        "  return jnp.einsum('ijk,ikl-\u003eijl', a, b)\n",
        "\n",
        "# Simply wrap the function in `jax.jit` to Just-In-Time compile it at runtime.\n",
        "g = jax.jit(f)\n",
        "\n",
        "print('Without jit.')\n",
        "%time _ = f(a, b)\n",
        "\n",
        "print('\\nWith jit (the first time it runs will be slower since the function is '\n",
        "      'being compiled.)')\n",
        "%time _ = g(a, b)\n",
        "\n",
        "print('\\nSubsequent runs with jit will be much faster.')\n",
        "%time _ = g(a, b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZ3AIEdV7PWe"
      },
      "source": [
        "### 5.2 Haiku overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7p8AJyaYoOSC"
      },
      "source": [
        "The JAX library ecosystem is large and constantly growing, with many new libraries being built using JAX as the backbone. One of the most popular libraries is **Haiku**, which is a neural network library for JAX.\n",
        "\n",
        "Neural network architectures are specified in the familiar layer-wise manner:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuMum31SoeOg"
      },
      "outputs": [],
      "source": [
        "def predict(inputs):\n",
        "  mlp = hk.Sequential([\n",
        "      hk.Linear(1024), jax.nn.relu,\n",
        "      hk.Linear(1024), jax.nn.relu,\n",
        "      hk.Linear(10), jax.nn.log_softmax,\n",
        "  ])\n",
        "  return mlp(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5L4WKIEuoQ4c"
      },
      "source": [
        "The main difference/trick of using JAX/Haiku for neural network code is that Haiku models must be defined within a transform. Haiku functions operate only on 'pure' functions whose output is fully determined by the arguments you pass in. For a neural network, the output not only depends on the input, but also the model parameters (for example, on the weights) which are normally implicit. Haiku provides a transformation function that turns functions into a pair of pure functions: one for initializing the parameters (`init`) and one for applying the function to an input given a set of parameters and an RNG key (`apply`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cB5KzwUaoiTm"
      },
      "outputs": [],
      "source": [
        "net = hk.transform(predict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kelKr7vzoYmX"
      },
      "source": [
        "The init method has to be applied to initialize the parameters of the network and return them. The init method takes a random `jax.random.PRNGKey` value and a sample input to inform the shape of the inputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xW42JLieok97"
      },
      "outputs": [],
      "source": [
        "rng = jax.random.PRNGKey(0)\n",
        "inputs = jax.random.uniform(rng, shape=(16, 1024), dtype=jnp.float32)\n",
        "params = net.init(rng, inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbc33p_uocUy"
      },
      "source": [
        "After that, we are ready to apply the forward functions to some inputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLNLFr0Aolmk"
      },
      "outputs": [],
      "source": [
        "preds = net.apply(params, None, inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ba5H_CFmTHJB"
      },
      "source": [
        "Don't worry if these JAX and Haiku concepts seem a bit abstract at the moment - you will see these principles in action in the next section, when we fine-tune a pre-trained convolutional network on the turtle face dataset!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcR01uCUTRTh"
      },
      "source": [
        "\u003ca name=\"Model\"\u003e\u003c/a\u003e\n",
        "## 6. Fine-tuning a ResNet model using JAX and Haiku"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdTVsZt3KydX"
      },
      "source": [
        "Time to get started with training our turtle facial recognition model!\n",
        "\n",
        "First, let's install and import some key libraries:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cDPMJuaKvU5"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp  # JAX version of numpy with a very similar API.\n",
        "try:\n",
        "  import haiku as hk\n",
        "except ModuleNotFoundError:\n",
        "  !pip install dm-haiku\n",
        "  import haiku as hk\n",
        "try:\n",
        "  import optax\n",
        "except ModuleNotFoundError:\n",
        "  !pip install optax\n",
        "  import optax\n",
        "try:\n",
        "  import immutabledict\n",
        "except ModuleNotFoundError:\n",
        "  !pip install immutabledict\n",
        "  import immutabledict\n",
        "import functools\n",
        "from PIL import Image  # Image utilities.\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_zcw0kqfRor"
      },
      "source": [
        "Create three mappings and get the paths to the training set image files.\n",
        "1. `labels` : turtle ID --\u003e unique integer labels\n",
        "1. `label_lookup` : unique integer labels --\u003e turtle ID\n",
        "1. `image_to_turtle` :  image IDs to turtle IDs (training set only).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ak6pcvg-f7bF"
      },
      "outputs": [],
      "source": [
        "turtle_ids = sorted(np.unique(train.turtle_id)) + ['new_turtle']\n",
        "labels = dict(zip(turtle_ids, np.arange(len(turtle_ids))))\n",
        "label_lookup = {v: k for k, v in labels.items()}\n",
        "num_classes = len(labels)\n",
        "image_to_turtle = dict(zip(train.image_id, train.turtle_id))\n",
        "\n",
        "image_files = [os.path.join(IMAGE_DIR, f) for f in os.listdir(IMAGE_DIR)\n",
        "              if f.split('.')[0] in train.image_id.values]\n",
        "\n",
        "image_ids = [os.path.basename(f).split('.')[0] for f in image_files]\n",
        "image_turtle_ids = [image_to_turtle[id] for id in image_ids]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esv-vvDGjLJ-"
      },
      "source": [
        "Load the training images into memory - takes a little while!\n",
        "\n",
        "*   Crops each image around the centre and resizes to `(224, 224)`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8_04m9w2j7x"
      },
      "outputs": [],
      "source": [
        "def crop_and_resize(pil_img):\n",
        "  \"\"\"Crops square from center of image and resizes to (224, 224).\"\"\"\n",
        "  w, h = pil_img.size\n",
        "  crop_size = min(w, h)\n",
        "  crop = pil_img.crop(((w - crop_size) // 2, (h - crop_size) // 2,\n",
        "                       (w + crop_size) // 2, (h + crop_size) // 2))\n",
        "  return crop.resize((224, 224))\n",
        "\n",
        "\n",
        "tqdm.tqdm._instances.clear()\n",
        "loaded_images = [crop_and_resize(Image.open(f)) for f in tqdm.tqdm(image_files)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zqpuZef9P67"
      },
      "source": [
        "Define a function to get a random batch of data from the training images:\n",
        "\n",
        "\n",
        "*   Randomly select `batch_size` elements from the available images\n",
        "*   Get the labels for the selected images\n",
        "*   Optionally rebalance the dataset so that every label is sampled uniformly\n",
        "\n",
        "\n",
        "Returns the batch of images of shape `(batch_size, 224, 224, 3)` and the integer labels of shape `(batch_size)`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4M0boTqq2Xv"
      },
      "outputs": [],
      "source": [
        "probability_per_label = {\n",
        "    label: 1 / label_count / len(train_images_per_turtle)\n",
        "    for label, label_count in train_images_per_turtle.items()\n",
        "}\n",
        "\n",
        "probabilities = [\n",
        "    probability_per_label[image_turtle_id]\n",
        "    for image_turtle_id in image_turtle_ids\n",
        "]\n",
        "assert np.isclose(1., np.sum(probabilities))\n",
        "\n",
        "\n",
        "def get_batch(batch_size, rebalance=False):\n",
        "  if rebalance:\n",
        "    probs = probabilities\n",
        "  else:\n",
        "    probs = None\n",
        "  batch_image_idxs = np.random.choice(\n",
        "      len(image_files), size=batch_size, replace=False, p=probs)\n",
        "  input_images = [loaded_images[idx] for idx in batch_image_idxs]\n",
        "  image_labels = [labels[image_turtle_ids[idx]] for idx in batch_image_idxs]\n",
        "  return (jnp.stack([\n",
        "      jnp.asarray(im, dtype=jnp.float32) / 255. for im in input_images\n",
        "  ]), jnp.stack(image_labels).astype(jnp.int32))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP_oB0SR_UwL"
      },
      "source": [
        "We can visualise what a batch of images looks like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkUWDVRc_RCU"
      },
      "outputs": [],
      "source": [
        "batch_images, _ = get_batch(batch_size=32)\n",
        "\n",
        "_, axes = plt.subplots(nrows=4, ncols=8, figsize=(12, 6))\n",
        "axes = axes.flatten()\n",
        "for img, ax in zip(list(batch_images), axes):\n",
        "  ax.imshow(img)\n",
        "  ax.xaxis.set_visible(False)\n",
        "  ax.yaxis.set_visible(False)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46o57JYv-Vrx"
      },
      "source": [
        "Define our network: a ResNet50 model made available via the Haiku library. The output is of size `num_classes`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFWrJX-4Wp4v"
      },
      "outputs": [],
      "source": [
        "@hk.without_apply_rng\n",
        "@hk.transform_with_state\n",
        "def resnet(x, is_training):\n",
        "  return hk.nets.ResNet50(\n",
        "      num_classes=num_classes, resnet_v2=True,\n",
        "      bn_config={'decay_rate': 0.9})(x, is_training)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNW8BEYS_tQ-"
      },
      "source": [
        "Next we define our loss and update functions.\n",
        "\n",
        "The loss function computes the softmax cross entropy between the set of logits and labels and sums over the batch. We also add L2 regularisation on the model\n",
        "parameters to help to alleviate overfitting.\n",
        "\n",
        "The update function computes the gradients and updates the parameters using the `jax.grad` and `optax.apply_updates` utility functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6LlSNAZ6P3e"
      },
      "outputs": [],
      "source": [
        "@functools.partial(jax.value_and_grad, has_aux=True)\n",
        "def loss_fn(params, state, inputs, labels):\n",
        "  predicted, new_state = net.apply(params, state, inputs, is_training=True)\n",
        "  predicted = jax.nn.log_softmax(predicted, axis=-1)\n",
        "  labels_one_hot = jax.nn.one_hot(labels, num_classes=num_classes)\n",
        "  loss = -(predicted * labels_one_hot).sum(axis=-1).mean()\n",
        "  loss = loss + l2_regularisation(params) * 0.005\n",
        "  return loss, new_state\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def update(params, state, opt_state, inputs, labels):\n",
        "  (loss, new_state), grads = loss_fn(params, state, inputs, labels)\n",
        "  updates, new_opt_state = opt.update(grads, opt_state)\n",
        "  new_params = optax.apply_updates(params, updates)\n",
        "  return new_params, new_state, new_opt_state, loss\n",
        "\n",
        "\n",
        "def l2_regularisation(params):\n",
        "  l2_norm = 0.\n",
        "  for module_name, module_params in params.items():\n",
        "    if 'batchnorm' not in module_name:\n",
        "      l2_norm += sum(\n",
        "          [jnp.sum(jnp.square(x)) for x in jax.tree_leaves(module_params)])\n",
        "  return l2_norm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KqMOSdGeO63"
      },
      "source": [
        "Now that our forward pass, loss function, and update function are defined, let's load in some pre-trained weights from a ResNet50 model trained on ImageNet:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bm3z91v_eO-E"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "checkpoint_url = urllib.parse.urljoin(BASE_URL,\n",
        "                                      'resnet50_imagenet_checkpoint.pystate')\n",
        "checkpoint = pickle.loads(requests.get(checkpoint_url).content)\n",
        "\n",
        "# Get model params and state from the checkpoint.\n",
        "pretrained_params = checkpoint['experiment_module']['params']\n",
        "pretrained_state = checkpoint['experiment_module']['state']\n",
        "pretrained_opt_state = checkpoint['experiment_module']['opt_state']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3790jJW8MPu"
      },
      "source": [
        "Most of the parameters of the pre-trained model are the same, except for the final layer:\n",
        "- The pretrained ResNet50 model was trained to predict 1000 output classes\n",
        "- Our current classification task has 101 output classes\n",
        "\n",
        "This means we can use all of the pretrained parameters expect for those in the final layer, which will have to be learned from scratch.\n",
        "\n",
        "We'll also need to update the optimiser state `opt_state` to reflect this change in final layer params."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGfzhTyN8yil"
      },
      "outputs": [],
      "source": [
        "import tree\n",
        "\n",
        "\n",
        "def update_params(path, values, scale_factor=-1e-5):\n",
        "  if path[-2:] == ('res_net50/~/logits', 'b'):\n",
        "    return scale_factor * jax.random.normal(\n",
        "        jax.random.PRNGKey(0), (num_classes,))\n",
        "  elif path[-2:] == ('res_net50/~/logits', 'w'):\n",
        "    return scale_factor * jax.random.normal(\n",
        "        jax.random.PRNGKey(0), (values.shape[0], num_classes))\n",
        "  else:\n",
        "    return values\n",
        "\n",
        "\n",
        "def update_opt_state(path, values):\n",
        "  if path[-2:] == ('res_net50/~/logits', 'b'):\n",
        "    return jnp.zeros((num_classes,))\n",
        "  elif path[-2:] == ('res_net50/~/logits', 'w'):\n",
        "    return jnp.zeros((values.shape[0], num_classes))\n",
        "  else:\n",
        "    return values\n",
        "\n",
        "\n",
        "pretrained_params = tree.map_structure_with_path(update_params,\n",
        "                                                 pretrained_params)\n",
        "pretrained_opt_state = tree.map_structure_with_path(update_opt_state,\n",
        "                                                    pretrained_opt_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGug8dT8nh7r"
      },
      "source": [
        "Fine tune the model using the pretrained parameters to warm start the model (here, we are using the Adam optimiser from `optax`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHDuycDGjH4c"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "finetuned_losses = []\n",
        "n_steps = 400\n",
        "opt = optax.adam(1e-3)\n",
        "opt_state = opt.init(pretrained_params)\n",
        "\n",
        "net = resnet\n",
        "\n",
        "for step in range(n_steps):\n",
        "  batch_images, batch_labels = get_batch(batch_size)\n",
        "  pretrained_params, pretrained_state, opt_state, loss = update(\n",
        "      pretrained_params, pretrained_state, opt_state, batch_images,\n",
        "      batch_labels)\n",
        "  finetuned_losses.append(loss)\n",
        "  if step % 50 == 0:\n",
        "    print(f\"Loss at step {step}: {loss:.3f}.\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-_A4zFioH4h"
      },
      "source": [
        "Plot the learning curve from this run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7vWUImCoJtP"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.arange(len(finetuned_losses)), finetuned_losses)\n",
        "plt.title(\"Fine-tuned model learning curve.\")\n",
        "plt.xlabel(\"Training steps\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovvlnoEMdjxr"
      },
      "source": [
        "We can also train a simple CNN network from scratch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrLj9r6h9qG2"
      },
      "outputs": [],
      "source": [
        "@hk.without_apply_rng\n",
        "@hk.transform_with_state\n",
        "def simple_cnn(x, is_training):\n",
        "\n",
        "  def conv_block(x, channels, kernel):\n",
        "    x = hk.Conv2D(\n",
        "        output_channels=channels,\n",
        "        kernel_shape=kernel,\n",
        "        stride=2,\n",
        "        padding='SAME',\n",
        "        with_bias=True)(\n",
        "            x)\n",
        "    x = jax.nn.relu(x)\n",
        "    return x\n",
        "\n",
        "  for channels, kernel in zip([8, 16], [3, 5]):\n",
        "    x = conv_block(x, channels, kernel)\n",
        "  x = hk.Flatten()(x)\n",
        "  x = hk.Linear(128)(x)\n",
        "  x = jax.nn.relu(x)\n",
        "  x = hk.Linear(num_classes)(x)\n",
        "  return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwmKhq30D_tJ"
      },
      "outputs": [],
      "source": [
        "# Initialise new network params, state, and optimiser from scratch.\n",
        "\n",
        "net = simple_cnn\n",
        "\n",
        "image_for_init, _ = get_batch(1)\n",
        "params, state = net.init(jax.random.PRNGKey(1), image_for_init, True)\n",
        "opt = optax.adam(1e-3)\n",
        "opt_state = opt.init(params)\n",
        "\n",
        "batch_size = 32\n",
        "from_scratch_losses = []\n",
        "n_steps = 1000\n",
        "\n",
        "for step in range(n_steps):\n",
        "  batch_images, batch_labels = get_batch(batch_size, rebalance=True)\n",
        "  params, state, opt_state, loss = update(params, state, opt_state,\n",
        "                                          batch_images, batch_labels)\n",
        "  from_scratch_losses.append(loss)\n",
        "  if step % 50 == 0:\n",
        "    print(f\"Loss at step {step}: {loss:.3f}.\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymIOtThP_sDK"
      },
      "source": [
        "Compare the learning curves from the two models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fPS_k-r_uYK"
      },
      "outputs": [],
      "source": [
        "figure, axes = plt.subplots(ncols=2, sharey=True, figsize=(10, 4))\n",
        "\n",
        "axes[0].plot(np.arange(len(finetuned_losses)), finetuned_losses)\n",
        "axes[0].set_title(\"Fine-tuning a pretrained model\")\n",
        "axes[0].set(xlabel=\"Training steps\", ylabel=\"Loss\")\n",
        "axes[1].plot(np.arange(len(from_scratch_losses)), from_scratch_losses)\n",
        "axes[1].set_title(\"Training from scratch\")\n",
        "axes[1].set(xlabel=\"Training steps\", ylabel=\"Loss\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZNsUNk3_uam"
      },
      "source": [
        "### 7. Performing inference using trained models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQjH3J_seK4Z"
      },
      "source": [
        "We can make predictions on new examples using a trained model by passing the model parameters, state and (preprocessed) inputs to the `apply` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnUCLMwFHOPt"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def predict(params, state, inputs):\n",
        "  \"\"\"Forward pass of model with log softmaxed output.\"\"\"\n",
        "  predicted, _ = net.apply(params, state, inputs, is_training=False)\n",
        "  return jax.nn.log_softmax(predicted, axis=-1)\n",
        "\n",
        "\n",
        "def get_image_by_image_id(image_id):\n",
        "  \"\"\"Function to get a model-ready image given an image ID\"\"\"\n",
        "  all_image_files = os.listdir(IMAGE_DIR)\n",
        "  all_image_ids = [\n",
        "      os.path.basename(file).split('.')[0] for file in all_image_files\n",
        "  ]\n",
        "  if image_id not in all_image_ids:\n",
        "    raise ValueError(f'Could not find image with ID {image_id}')\n",
        "  image_filepath = all_image_files[all_image_ids.index(image_id)]\n",
        "  image = Image.open(os.path.join(IMAGE_DIR, image_filepath))\n",
        "  image = crop_and_resize(image)\n",
        "  return jnp.stack([jnp.asarray(image, dtype=jnp.float32) / 255.])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JSRREsY27A3"
      },
      "source": [
        "Now, given a new image ID like `ID_092X1NP4`, we can predict the corresponding most likely turtle ID:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlubBY1X26N3"
      },
      "outputs": [],
      "source": [
        "logits = predict(params, state, get_image_by_image_id('ID_092X1NP4'))\n",
        "logits = jax.device_get(logits)\n",
        "predicted_turtle_id = label_lookup[int(np.argsort(logits)[0][-1])]\n",
        "\n",
        "print(\n",
        "    f'The top prediction for image ID \"ID_092X1NP4\" is \"{predicted_turtle_id}\".'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTgdDd0PDsI9"
      },
      "source": [
        "## 8. Generating test set predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P30YUCZ-3mjU"
      },
      "source": [
        "We can generate predictions for the entire test set by simply calling `predict` on each example. Let's first load the test images and apply the same cropping and resizing as before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciQ83LaAEA2p"
      },
      "outputs": [],
      "source": [
        "tqdm.tqdm._instances.clear()\n",
        "test_image_files = [os.path.join(IMAGE_DIR, f) for f in os.listdir(IMAGE_DIR)\n",
        "                    if f.split('.')[0] in test.image_id.values]\n",
        "test_image_ids = [os.path.basename(f).split('.')[0] for f in test_image_files]\n",
        "loaded_test_images = [crop_and_resize(Image.open(f)) for f in tqdm.tqdm(test_image_files)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRho_jz330gB"
      },
      "source": [
        "The following utilities will perform batch inference and format the results. For submission we need a csv with an `image_id` column, and separate columns for each of our top 5 predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jl3sImdxwgOu"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "\n",
        "def batch_list(list_to_batch, batch_size):\n",
        "  \"\"\"Chunk up a list into batches, potentially with a smaller final batch.\"\"\"\n",
        "  return [list_to_batch[i:i + batch_size] for i in range(\n",
        "      0, len(list_to_batch), batch_size)]\n",
        "\n",
        "def predict_on_set(batched_image_ids,\n",
        "                   batched_images,\n",
        "                   params, state):\n",
        "  \"\"\"Returns top 5 predictions on batched images as a submission dataframe.\"\"\"\n",
        "  model_predictions = []\n",
        "\n",
        "  for ids, images in zip(batched_image_ids, batched_images):\n",
        "    # Stack images for batch inference.\n",
        "    images = jnp.stack(\n",
        "        [jnp.asarray(im, dtype=jnp.float32) / 255. for im in images])\n",
        "\n",
        "    # Make predictions and sort logits to find top 5 predictions.\n",
        "    logits = predict(params, state, images)\n",
        "    logits = jax.device_get(logits)\n",
        "    top_5_predictions = np.argsort(logits)[:, -5:][:, ::-1]\n",
        "\n",
        "    # Format results.\n",
        "    for image_id, predictions in zip(ids, top_5_predictions):\n",
        "      row = {}\n",
        "      predicted_turtle_ids = [label_lookup[label] for label in predictions]\n",
        "      row['image_id'] = image_id\n",
        "      for prediction_idx, prediction in enumerate(predicted_turtle_ids):\n",
        "        row[f'prediction{prediction_idx + 1}'] = prediction\n",
        "      model_predictions.append(row)\n",
        "\n",
        "  return pd.DataFrame(model_predictions).set_index('image_id')\n",
        "\n",
        "# Batch up test images and IDs.\n",
        "eval_batch_size = 32\n",
        "test_batched_images = batch_list(loaded_test_images, eval_batch_size)\n",
        "test_batched_image_ids = batch_list(test_image_ids, eval_batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-YxvWUDxN_E"
      },
      "outputs": [],
      "source": [
        "net = simple_cnn\n",
        "predictions_from_scratch = predict_on_set(test_batched_image_ids,\n",
        "                                          test_batched_images,\n",
        "                                          params, state)\n",
        "net = resnet\n",
        "predictions_from_pretrained = predict_on_set(test_batched_image_ids,\n",
        "                                             test_batched_images,\n",
        "                                             pretrained_params,\n",
        "                                             pretrained_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWB1fahrZIvZ"
      },
      "source": [
        "\u003ca name=\"Submit\"\u003e\u003c/a\u003e\n",
        "## 9. Submitting our predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dTRlGxo4Amj"
      },
      "source": [
        "Now our predictions are ready, we can save them to file and download them, ready to submit to Zindi:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1UzR9gq4FGE"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "predictions_from_scratch.to_csv('submission.csv')\n",
        "files.download('submission.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJWEXsLwfUy-"
      },
      "source": [
        "### The evaluation metric: Mean Average Precision (`MAP@5`)\n",
        "\n",
        "We are using Mean Average Precision at 5, where 5 refers to the number of predictions submitted for each turtle.\n",
        "\n",
        "\n",
        "#### Precision @k (`P@k`)\n",
        "\n",
        "- Defined as `true_positives_in_top_k_predictions / k `, this captures how many relevant items are present in the top `k` recommendations of your system.\n",
        "\n",
        "- For example, let's assume the the prediction of one row is as follows:\n",
        "\n",
        "  ```actual = \"t_id_ROFhVsy2\"```\n",
        "\n",
        "  ``` predicted = [\"t_id_ROFhVsy2\", \"t_id_UVQa4BMz\", \"t_id_a4VYrmyA\", \"new_turtle\", \"t_id_4ZfTUmwL\"]```\n",
        "\n",
        "  Then, for different values of `k`:\n",
        "\n",
        "  `P@1  = 1/1 `\n",
        "\n",
        "  `P@2 = 1/2 `\n",
        "\n",
        "  `P@3 = 1/3`\n",
        "\n",
        "  `P@4 = 1/4`\n",
        "\n",
        "  `P@5 = 1/5`\n",
        "\n",
        "  \u003cbr\u003e\n",
        "\n",
        "#### Average precision @ k (`AP@k`)\n",
        "- Defined as the mean of `P@i` for `i=1, ..., K`.\n",
        "\n",
        "\u003cbr\u003e\n",
        "\n",
        "#### Mean Average Precision @k (`MAP@K`)\n",
        "- Defined as the mean of the `AP@K` for all the turtles.\n",
        "\n",
        "- For our metric, `MAP@5`: sum `AP@5` for all the turtles and divide that value by the number of turtles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv2m6J041L-e"
      },
      "source": [
        "Here are functions that you can use to calculate the `AP@5` and `MAP@5` for a given label and list of predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWZ6OIUW1YDn"
      },
      "outputs": [],
      "source": [
        "def apk(actual, predicted, k=5):\n",
        "  \"\"\"Computes the average precision at k.\n",
        "\n",
        "  Args:\n",
        "    actual: The turtle ID to be predicted.\n",
        "    predicted : A list of predicted turtle IDs (order does matter).\n",
        "    k : The maximum number of predicted elements.\n",
        "\n",
        "  Returns:\n",
        "    The average precision at k.\n",
        "  \"\"\"\n",
        "  if len(predicted) \u003e k:\n",
        "    predicted = predicted[:k]\n",
        "\n",
        "  score = 0.0\n",
        "  num_hits = 0.0\n",
        "\n",
        "  for i, p in enumerate(predicted):\n",
        "    if p == actual and p not in predicted[:i]:\n",
        "      num_hits += 1.0\n",
        "      score += num_hits / (i + 1.0)\n",
        "\n",
        "  return score\n",
        "\n",
        "\n",
        "def mapk(actual, predicted, k=5):\n",
        "  \"\"\" Computes the mean average precision at k.\n",
        "\n",
        "    The turtle ID at actual[i] will be used to score predicted[i][:k] so order\n",
        "    matters throughout!\n",
        "\n",
        "    actual: A list of the true turtle IDs to score against.\n",
        "    predicted: A list of lists of predicted turtle IDs.\n",
        "    k: The size of the window to score within.\n",
        "\n",
        "    Returns:\n",
        "      The mean average precision at k.\n",
        "  \"\"\"\n",
        "  return np.mean([apk(a, p, k) for a, p in zip(actual, predicted)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRXRDtflz2xP"
      },
      "outputs": [],
      "source": [
        "predictions = predictions_from_scratch[[\n",
        "    \"prediction1\", \"prediction2\", \"prediction3\", \"prediction4\", \"prediction5\"\n",
        "]]\n",
        "y_predict = predictions.values.tolist()\n",
        "\n",
        "# We don't actually know the true labels for the test set, so for the purposes\n",
        "# of demonstration we just assume that all of the images in the test set are of\n",
        "# a single turtle:\n",
        "assumed_y = [\"t_id_d6aYXtor\"] * len(y_predict)\n",
        "\n",
        "mapk_result = mapk(assumed_y, y_predict, k=5)\n",
        "print(\"With made up test set labels, our mapk with k=5 is\", mapk_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0zA8pLH36rd"
      },
      "source": [
        "### Generalisability prize\n",
        "Please note that there is an additional prize for generalisability: The likelihood of the approach and algorithm being able to generalise beyond the challenge dataset without frequent re-training, taking into account the approach and algorithm used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sikpOFzXZIyG"
      },
      "source": [
        "\u003ca name=\"Suggestions\"\u003e\u003c/a\u003e\n",
        "## 8. Suggestions for improving the model\n",
        "\n",
        "This tutorial provides a jumping off point for this task, but there are plenty of improvements you might consider making to the model presented in this colab. A few possible directions to explore:\n",
        "- **Data-related**\n",
        "  - Preprocessing the dataset in various ways - cropping, colour-correction, etc.\n",
        "  - Augmenting the dataset - flipping, rotating, tinting, etc. the images to increase data size and potentially improve model generalisation\n",
        "  - Making use of the extra images, listed in `extra_images.csv`, provided in the dataset\n",
        "- **Model-related**\n",
        "  - Outputting 'new_turtle' when the model is particularly uncertain\n",
        "  - Trying different pre-trained models\n",
        "  - Playing around with the model architecture\n",
        "  - Trying model ensembling\n",
        "  - Hyperparameter tuning\n",
        "  - Other regularisation approaches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRShUBH3oMhr"
      },
      "source": [
        "Thanks for reading, hope you enjoyed this tutorial and we're looking forward to seeing your entries!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wxw2ySGTm24"
      },
      "source": [
        "\u003ca name=\"Legal\"\u003e\u003c/a\u003e\n",
        "## 9. License and Disclaimer\n",
        "\n",
        "This is not an officially-supported Google product.\n",
        "\n",
        "Copyright 2021 DeepMind Technologies Limited.\n",
        "\n",
        "This notebook and code is licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0.\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "\n",
        "**Model Parameters License**\n",
        "\n",
        "The pre-trained model parameters are made available under the terms of the Creative Commons Attribution 4.0 International (CC BY 4.0) license. You can find details at: https://creativecommons.org/licenses/by/4.0/legalcode\n",
        "\n",
        "**Data Set**\n",
        "\n",
        "The data set of turtle images and associated labels have been provided by Zindi and Local Ocean Conservation.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "[Tutorial]_Turtle_Recall_Conservation_Challenge.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
