{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"turtleRecallColab.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM2+UMecZJuuaF0wO0gP+7k"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Turtle Recall\n","A facial recognition model for turtles\n","\n","https://zindi.africa/competitions/turtle-recall-conservation-challenge/data"],"metadata":{"id":"u16065IenPrx"}},{"cell_type":"code","source":["!pip install tensorflow-addons"],"metadata":{"id":"nGjCcZB0qz0A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","import tensorflow_addons as tfa\n","import tensorflow_hub as hub\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import os\n","import requests\n","import io\n","import urllib.parse\n","import tqdm\n","import datetime\n","from PIL import Image"],"metadata":{"id":"vd4LF05Vcxf5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'TensorFlow version is {tf.__version__}')"],"metadata":{"id":"K4KJMuIpc7RN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["physical_devices = tf.config.list_physical_devices('GPU')\n","print(\"Num GPUs:\", len(physical_devices))"],"metadata":{"id":"AG0_iPDOc_ty"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%load_ext tensorboard"],"metadata":{"id":"EFZgYunKm7dw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Dataset\n","\n","First, we load the data. In addition to the turtles and images from the train.csv file, we also make use of extra_images.csv by concatenation with the train file. This yields substantially more (ca. 10.000) image files to later train the model on."],"metadata":{"id":"DCXvR3mid-ra"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5nnUvoYXchLr"},"outputs":[],"source":["SOURCE_URL = 'https://storage.googleapis.com/dm-turtle-recall/images.tar'\n","IMAGE_DIR = './data/images'\n","TAR_PATH = os.path.join(IMAGE_DIR, os.path.basename(SOURCE_URL))\n","EXPECTED_IMAGE_COUNT = 13891\n","\n","%sx mkdir --parents \"{IMAGE_DIR}\"\n","if len(os.listdir(IMAGE_DIR)) != EXPECTED_IMAGE_COUNT:\n","  %sx wget --no-check-certificate -O \"{TAR_PATH}\" \"{SOURCE_URL}\"\n","  %sx tar --extract --file=\"{TAR_PATH}\" --directory=\"{IMAGE_DIR}\"\n","  %sx rm \"{TAR_PATH}\"\n","\n","print(f'The total number of images is: {len(os.listdir(IMAGE_DIR))}')"]},{"cell_type":"code","source":["BASE_URL = 'https://storage.googleapis.com/dm-turtle-recall/'\n","\n","def read_csv_from_web(file_name):\n","  url = urllib.parse.urljoin(BASE_URL, file_name)\n","  content = requests.get(url).content\n","  return pd.read_csv(io.StringIO(content.decode('utf-8')))\n","\n","\n","# Read in csv files.\n","train = read_csv_from_web('train.csv')\n","extra_images = read_csv_from_web('extra_images.csv')\n","\n","# Convert image_location strings to lowercase.\n","for row in [train]:\n","  row.image_location = row.image_location.apply(lambda x: x.lower())\n","  assert set(row.image_location.unique()) == set(['left', 'right', 'top'])\n","\n","df = pd.concat(objs=[train, extra_images])"],"metadata":{"id":"yQVB4a3edDep"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["images_per_turtle = pd.value_counts(df.turtle_id)\n","print(f'The total number of turtles is {len(df.turtle_id.unique())}.\\n'\n","      'The mean number of training images per turtle is '\n","      f'{round(np.mean(images_per_turtle), 2)}, '\n","      f'and the median is {int(np.median(images_per_turtle))}.')"],"metadata":{"id":"YQMgWAfDd09C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As we can see, however, we don't get a lot of images per turtle on average. Actually, some 2000 turtles are represented with less than 10 images in the dataset, which leads to a huge imbalance. Hence, we decide not to make use of any turtle with less than `MIN_NR_IMGS`."],"metadata":{"id":"GUBszaAGd5Ns"}},{"cell_type":"code","source":["MIN_NR_IMGS = 10\n","\n","im_per_turtle = images_per_turtle[images_per_turtle >= MIN_NR_IMGS].to_frame()\n","df = df[df.turtle_id.isin(im_per_turtle.index)].reset_index()"],"metadata":{"id":"kNBEQK65d3Jp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["images_per_turtle = pd.value_counts(df.turtle_id)\n","print(f'The total number of turtles after removal is {len(df.turtle_id.unique())}.\\n'\n","      'The mean number of training images per turtle is now '\n","      f'{round(np.mean(images_per_turtle), 2)}, '\n","      f'and the median is {int(np.median(images_per_turtle))}. \\n'\n","      f'The smallest number of images per turtle is '\n","      f'{min(df.turtle_id.value_counts())}.')"],"metadata":{"id":"Lm3xmoxHeHVs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(df.shape)\n","df.head(3)"],"metadata":{"id":"fg8zAgQyeJjw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We have now removed a significant portion of the data and are left with about 5000 images, which is still more than double the amount of the initial images in the `train.csv`. There is, however, still a huge imbalance in the dataset and the total number of files is quite small."],"metadata":{"id":"16RBUVpqeNWe"}},{"cell_type":"code","source":["plt.hist(x=images_per_turtle, rwidth=0.9, bins=20)\n","plt.xlabel('Images per train turtle')\n","plt.show()"],"metadata":{"id":"-bRAwopLeOI-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Preprocessing\n","\n","We create mappings and get the paths to the image files. After that follows some basic and some advanced preprocessing."],"metadata":{"id":"zATi6kSVeScm"}},{"cell_type":"code","source":["turtle_ids = sorted(np.unique(df.turtle_id)) + ['new_turtle']\n","\n","image_files = [os.path.join(IMAGE_DIR, f) for f in os.listdir(IMAGE_DIR) if f.split('.')[0] in df.image_id.values]\n","image_ids = [os.path.basename(f).split('.')[0] for f in image_files]\n","\n","image_to_turtle = dict(zip(df.image_id, df.turtle_id))\n","labels = dict(zip(turtle_ids, np.arange(len(turtle_ids))))\n","\n","loaded_labels = [labels[image_to_turtle[id]] for id in image_ids]"],"metadata":{"id":"grtgEDLcePCR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["NUM_CLASSES = len(turtle_ids)"],"metadata":{"id":"w9wzs0wXeaoF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def crop_and_resize(pil_img, img_size=(224,224)):\n","  \"\"\"Crop square from center of image and resize.\"\"\"\n","\n","  w, h = pil_img.size\n","  crop_size = min(w, h)\n","  crop = pil_img.crop(((w - crop_size) // 2, (h - crop_size) // 2,\n","                       (w + crop_size) // 2, (h + crop_size) // 2))\n","  \n","  return crop.resize(img_size)\n","\n","tqdm.tqdm._instances.clear()\n","loaded_images = [crop_and_resize(Image.open(f)) for f in tqdm.tqdm(image_files)]"],"metadata":{"id":"23jMmkOneei3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# inspect an image\n","print(loaded_images[0].size)\n","print(len(loaded_images))\n","loaded_images[0]"],"metadata":{"id":"bch1-teperQs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ims = tf.stack([tf.convert_to_tensor(np.asarray(im), dtype=tf.float32) for im in loaded_images])\n","labels = tf.stack(loaded_labels)"],"metadata":{"id":"iosDj-3RevN5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_ds = tf.data.Dataset.from_tensor_slices((ims, labels))\n","train_ds = train_ds.map(lambda x,y: (x/255., tf.one_hot(y, NUM_CLASSES)))\n","\n","print(f'The dataset contains {train_ds.cardinality().numpy()} images.')"],"metadata":{"id":"jJH2ffPeewto"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data augmentation\n","\n","Before applying augmentation to our images and hence increasing the size of our training data, we shuffle the current dataset, take a few images and store them in a test set for eventually evaluating our model. We do this to preserve the real-world data we want our model to work on later. The augmentation is then only used on training and validation data to make sure our model learns with a variety of different images and is robust against noise, different colour and brightness values, etc."],"metadata":{"id":"gk1yFvxVeytO"}},{"cell_type":"code","source":["BUFFER = train_ds.cardinality().numpy()\n","TEST_SPLIT = 1000\n","\n","train_ds = train_ds.shuffle(buffer_size=BUFFER, reshuffle_each_iteration=False)\n","test_ds, train_ds = train_ds.take(TEST_SPLIT), train_ds.skip(TEST_SPLIT)\n","\n","print(\n","    f'Train images: {train_ds.cardinality().numpy()}', \n","    f'Test images: {test_ds.cardinality().numpy()}', \n","    sep='\\n')"],"metadata":{"id":"BoYkGr4neyeo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Augmentation functions"],"metadata":{"id":"jKak5O25qW3I"}},{"cell_type":"code","source":["def rotate_images(ds):\n","    \"\"\"\n","    Rotates images by 90, 180, and 270 degrees.\n","    Quadruples size of dataset.\n","    \"\"\"\n","\n","    ds_rotated_90 = ds.map(lambda x,y: (tfa.image.rotate(x, angles=0.5*np.pi), y))\n","    ds_rotated_180 = ds.map(lambda x,y: (tfa.image.rotate(x, angles=np.pi), y))\n","    ds_rotated_270 = ds.map(lambda x,y: (tfa.image.rotate(x, angles=1.5*np.pi), y))\n","\n","    ds = ds_rotated_90.concatenate(ds_rotated_180).concatenate(ds_rotated_270)\n","\n","    return ds"],"metadata":{"id":"GIF-1LjnqWFp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def apply_gaussian_filter(ds, filter_shape=7, sigma=2):\n","    \"\"\"\n","    Apply a Gaussian image blur. Doubles the size of the input dataset.\n","    \"\"\"\n","\n","    ds_gaussian = ds.map(lambda x,y: (tfa.image.gaussian_filter2d(x, filter_shape=filter_shape, sigma=sigma), y))\n","    return ds_gaussian"],"metadata":{"id":"Z1n_mk48qgfB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def random_hsv(ds):\n","    \"\"\"\n","    Randomly adjust hue, saturation, value of an RGB image in the YIQ color space.\n","    \"\"\"\n","\n","    ds_hsv = ds.map(lambda x,y: (tfa.image.random_hsv_in_yiq(x, max_delta_hue=0.8, lower_saturation=0.2, upper_saturation=0.8, lower_value=0.2, upper_value=0.8), y))\n","    return ds_hsv\n"],"metadata":{"id":"6boxozsaqiWi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def add_noise(ds, sd=0.2):\n","    \"\"\"\n","    Additive noise\n","    \"\"\"\n","\n","    ds_noise = ds.map(lambda x,y: (x + tf.random.normal(x.shape, mean=0.0, stddev=sd, dtype=tf.float32), y))\n","    ds_noise = ds_noise.map(lambda x,y: (tf.clip_by_value(x, 0.0, 1.0), y))\n","\n","    return ds_noise"],"metadata":{"id":"2whCf0t2qkFx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If desired, single augmentations can be added to the dataset by concatenation."],"metadata":{"id":"l8viC4k8I5cX"}},{"cell_type":"code","source":["ds_rotated = rotate_images(train_ds)\n","ds_gaussian = apply_gaussian_filter(train_ds)\n","ds_hsv = random_hsv(train_ds)\n","ds_noise = add_noise(train_ds, 0.2)\n","\n","#train_ds = train_ds.concatenate(ds_rotated).concatenate(ds_gaussian).concatenate(ds_hsv).concatenate(ds_noise)\n"],"metadata":{"id":"7okk6VTQqoCG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["BATCH_SIZE = 64\n","TRAIN_SPLIT = np.round(train_ds.cardinality().numpy() * 0.85)\n","\n","train_ds, val_ds = train_ds.take(TRAIN_SPLIT), train_ds.skip(TRAIN_SPLIT)\n","\n","train_ds = train_ds.shuffle(2048).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n","train_ds = train_ds.cache(filename='cached_train_ds')\n","\n","val_ds = val_ds.shuffle(1024).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n","val_ds = val_ds.cache(filename='cached_val_ds')\n","\n","test_ds = test_ds.shuffle(1024).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n","test_ds = test_ds.cache(filename='cached_test_ds')\n","\n","print(\n","    f'Training dataset contains {train_ds.cardinality().numpy() * BATCH_SIZE} images after data augmentation.',\n","    f'Validation dataset contains {val_ds.cardinality().numpy() * BATCH_SIZE} images.',\n","    sep='\\n')"],"metadata":{"id":"iDRdVyN8e5nJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"fDjNuMICe7r8"}},{"cell_type":"code","source":["tf.keras.backend.clear_session()\n","NR_EPOCHS = 10"],"metadata":{"id":"hRgzRuh4V0-l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## EfficientNetV2\n","\n","Now we bring in a pre-trained EfficientNetV2-B0, a fairly new architecture that has been build to be extremely efficient and transfer well."],"metadata":{"id":"jhDZ4_bzMct-"}},{"cell_type":"code","source":["hub_url = \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b0/feature_vector/2\"\n","\n","efficientNet = tf.keras.Sequential([\n","    tf.keras.layers.InputLayer(input_shape=(224,224,3)),\n","    hub.KerasLayer(hub_url, trainable=True),\n","    tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n","])\n","\n","efficientNet.summary()"],"metadata":{"id":"3NHMcsbje9GD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["efficientNet.compile(\n","    optimizer=tf.keras.optimizers.Adam(),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[tf.keras.metrics.CategoricalAccuracy(), tf.keras.metrics.Precision(top_k=5)]\n",")\n","\n","log_dir = \"logs/efficientNetV2\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"],"metadata":{"id":"H2Dcqqhbe_X4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%tensorboard --logdir=logs"],"metadata":{"id":"QKT-CL-zlKh6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["efficientNet.fit(train_ds,\n","                 epochs=NR_EPOCHS, \n","                 validation_data=val_ds,\n","                 callbacks=[tensorboard_callback])"],"metadata":{"id":"ORX6SmQ7fAvp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["efficientNet.evaluate(test_ds)"],"metadata":{"id":"2FxH4PhFfCHr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## InceptionV3\n","\n","InceptionV3 CNN as per Szegedy et al. (2015)."],"metadata":{"id":"j3UJczy7QMr-"}},{"cell_type":"code","source":["hub_url = \"https://tfhub.dev/google/inaturalist/inception_v3/feature_vector/5\"\n","\n","inception = tf.keras.Sequential([\n","    tf.keras.layers.InputLayer(input_shape=(224,224,3)),\n","    hub.KerasLayer(hub_url, trainable=True),\n","    tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n","])\n","\n","inception.summary()"],"metadata":{"id":"2puIziwBQPMT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%tensorboard --logdir=logs"],"metadata":{"id":"cNOvSx9LQaie"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inception.compile(\n","    optimizer=tf.keras.optimizers.Adam(),\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=[tf.keras.metrics.CategoricalAccuracy(), tf.keras.metrics.Precision(top_k=5)]\n",")\n","\n","log_dir = \"logs/inceptionV3\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"],"metadata":{"id":"ngkgGEpmQXwk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inception.fit(train_ds,\n","                 epochs=NR_EPOCHS, \n","                 validation_data=val_ds,\n","                 callbacks=[tensorboard_callback])"],"metadata":{"id":"K6kYAJvXQcWb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inception.evaluate(test_ds)"],"metadata":{"id":"F5t8gukkp6zB"},"execution_count":null,"outputs":[]}]}