\section{Open Questions}

We consider achieving a test accuracy of $63.80\%$ given the size and characteristics of the dataset after 10 epochs of training already a great success, but we could not evaluate the models for their full potential within the scope of this project. With more computational resources available, the next obvious step would be to increase the size of the dataset to include all augmentations while also including more turtles again by lowering the number of minimum images per turtle. Then, it would also be interesting to conduct a more thorough hyperparameter search, which was not feasible for us. For example, we only used Adam with standard parameters as optimizer because it is computationally efficient and requires little memory \citep{Kingma2014}. The EfficientNet model family has been trained with Hinton's unpublished RMSProp \citep{Tan2019}. Further, we only reported categorical accuracy as model evaluation metric. In the original Zindi competition, mean average precision at top 5 (MAP@5) has been suggested for this problem, probably because of the many classes to consider. Another interesting approach could be a less aggressive downsampling of the images. As proposed in the original competition, we scaled the images down to a resolution of $224\times224$, exactly the resolution that is also suggested as input for the EfficientNetV2-B0. As already discussed earlier, EfficientNet is a model family and B0 the smallest one in the ensemble. By scaling up the image resolution, it would not only be possible to include more information in the images themselves, but also to employ a larger and stronger model.

Further, we manually designed the augmentation. Doing so effectively is usually dataset-specific and requires expert knowledge in the image domain, as different techniques work best for different domains. \citeauthor{Cubuk2018} have proposed their AutoAugment image augmentation procedure in \citeyear{Cubuk2018}, in which a search space of augmentations is automatically explored to find the best procedures given the data at hand.

The competition on Zindi is not yet over at the time of writing this documentation, but the top-5 leaderboard scores already achieve a MAP@5 of more than $98\%$. We are excited to see which procedure ultimately works best on this problem.