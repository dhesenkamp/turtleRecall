{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turtle Recall\n",
    "A facial recognition model for turtles\n",
    "\n",
    "https://zindi.africa/competitions/turtle-recall-conservation-challenge/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_hub as hub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import tqdm\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'TensorFlow version is {tf.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs:\", len(physical_devices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the data. In addition to the turtles and images from the train.csv file, we also make use of extra_images.csv by concatenation with the train file. This yields substantially more (ca. 10.000) image files to later train the model on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIR = './data/images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./data/train.csv\")\n",
    "extra_images = pd.read_csv(\"./data/extra_images.csv\")\n",
    "\n",
    "# Convert image_location strings to lowercase.\n",
    "for row in [train]:\n",
    "  row.image_location = row.image_location.apply(lambda x: x.lower())\n",
    "  assert set(row.image_location.unique()) == set(['left', 'right', 'top'])\n",
    "\n",
    "df = pd.concat(objs=[train, extra_images])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_per_turtle = pd.value_counts(df.turtle_id)\n",
    "print(f'The total number of turtles is {len(df.turtle_id.unique())}.\\n'\n",
    "      'The mean number of training images per turtle is '\n",
    "      f'{round(np.mean(images_per_turtle), 2)}, '\n",
    "      f'and the median is {int(np.median(images_per_turtle))}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, however, we don't get a lot of images per turtle on average. Actually, some 2000 turtles are represented with less than 10 images in the dataset, which leads to a huge imbalance. Hence, we decide not to make use of any turtle with less than `MIN_NR_IMGS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_NR_IMGS = 10\n",
    "\n",
    "im_per_turtle = images_per_turtle[images_per_turtle >= MIN_NR_IMGS].to_frame()\n",
    "df = df[df.turtle_id.isin(im_per_turtle.index)].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_per_turtle = pd.value_counts(df.turtle_id)\n",
    "print(f'The total number of turtles after removal is {len(df.turtle_id.unique())}.\\n'\n",
    "      'The mean number of training images per turtle is now '\n",
    "      f'{round(np.mean(images_per_turtle), 2)}, '\n",
    "      f'and the median is {int(np.median(images_per_turtle))}. \\n'\n",
    "      f'The smallest number of images per turtle is '\n",
    "      f'{min(df.turtle_id.value_counts())}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now removed a significant portion of the data and are left with about 5000 images, which is still more than double the amount of the initial images in the `train.csv`. There is, however, still a huge imbalance in the dataset and the total number of files is quite small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(x=images_per_turtle, rwidth=0.9, bins=20)\n",
    "plt.xlabel('Images per train turtle')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create three mappings and get the paths to the training set image files.\n",
    "1. `labels` : turtle ID --> unique integer labels\n",
    "1. `label_lookup` : unique integer labels --> turtle ID\n",
    "1. `image_to_turtle` :  image IDs to turtle IDs (training set only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required by current pipeline\n",
    "turtle_ids = sorted(np.unique(df.turtle_id)) + ['new_turtle']\n",
    "\n",
    "image_files = [os.path.join(IMAGE_DIR, f) for f in os.listdir(IMAGE_DIR) if f.split('.')[0] in df.image_id.values]\n",
    "image_ids = [os.path.basename(f).split('.')[0] for f in image_files]\n",
    "\n",
    "image_to_turtle = dict(zip(df.image_id, df.turtle_id))\n",
    "labels = dict(zip(turtle_ids, np.arange(len(turtle_ids))))\n",
    "\n",
    "loaded_labels = [labels[image_to_turtle[id]] for id in image_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tutorial.ipynb, but not currently used in pipeline\n",
    "label_lookup = {v: k for k, v in labels.items()}\n",
    "num_classes = len(labels)\n",
    "\n",
    "image_turtle_ids = [image_to_turtle[id] for id in image_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = len(turtle_ids)\n",
    "NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_resize(pil_img, img_size=(224,224)):\n",
    "  \"\"\"Crop square from center of image and resize.\"\"\"\n",
    "\n",
    "  w, h = pil_img.size\n",
    "  crop_size = min(w, h)\n",
    "  crop = pil_img.crop(((w - crop_size) // 2, (h - crop_size) // 2,\n",
    "                       (w + crop_size) // 2, (h + crop_size) // 2))\n",
    "  \n",
    "  return crop.resize(img_size)\n",
    "\n",
    "tqdm.tqdm._instances.clear()\n",
    "loaded_images = [crop_and_resize(Image.open(f)) for f in tqdm.tqdm(image_files)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect an image\n",
    "print(loaded_images[0].size)\n",
    "print(len(loaded_images))\n",
    "loaded_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ims = tf.stack([tf.convert_to_tensor(np.asarray(im), dtype=tf.float32) for im in loaded_images])\n",
    "labels = tf.stack(loaded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices((ims, labels))\n",
    "ds = ds.map(lambda x,y: (x/255., tf.one_hot(y, NUM_CLASSES)))\n",
    "\n",
    "print(f'The dataset contains {ds.cardinality().numpy()} images.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation\n",
    "https://colab.research.google.com/github/tensorflow/addons/blob/master/docs/tutorials/image_ops.ipynb#scrollTo=tbaIkUCS2eNv\n",
    "\n",
    "Before applying augmentation to our images and hence increasing the size of our training data, we shuffle the current dataset, take a few images and store them in a test set for eventually evaluating our model. We do this to preserve the real-world data we want our model to work on later. The augmentation is then only used on training and validation data to make sure our model learns with a variety of different images and is robust against noise, different colour and brightness values, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER = ds.cardinality().numpy()\n",
    "TEST_SPLIT = 1500\n",
    "\n",
    "ds = ds.shuffle(buffer_size=BUFFER, reshuffle_each_iteration=False)\n",
    "test_ds, train_ds = ds.take(TEST_SPLIT), ds.skip(TEST_SPLIT)\n",
    "\n",
    "print(f'Train images: {train_ds.cardinality().numpy()}', f'Test images: {test_ds.cardinality().numpy()}', sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implemented a bunch of augmentation functions to deal with the small dataset. After applying them, we further split off a validation part from the training images which we can use during the training process to assess the progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.image_augmentation import rotate_images, apply_mean_filter, apply_gaussian_filter, random_hsv, add_noise\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "ds_rotated = rotate_images(train_ds)\n",
    "ds_gaussian = apply_gaussian_filter(train_ds)\n",
    "ds_hsv = random_hsv(train_ds)\n",
    "ds_noise = add_noise(train_ds, 0.2)\n",
    "\n",
    "train_ds = train_ds.concatenate(ds_rotated).concatenate(ds_gaussian).concatenate(ds_hsv).concatenate(ds_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SPLIT = np.round(train_ds.cardinality().numpy() * 0.9)\n",
    "\n",
    "train_ds, val_ds = train_ds.take(TRAIN_SPLIT), train_ds.skip(TRAIN_SPLIT)\n",
    "\n",
    "train_ds = train_ds.shuffle(2048).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "train_ds = train_ds.cache(filename='cached_train_ds')\n",
    "\n",
    "val_ds = val_ds.shuffle(1024).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "val_ds = val_ds.cache(filename='cached_val_ds')\n",
    "\n",
    "print(\n",
    "    f'Training dataset contains {train_ds.cardinality().numpy() * BATCH_SIZE} images after data augmentation.',\n",
    "    f'Validation dataset contains {val_ds.cardinality().numpy() * BATCH_SIZE} images.',\n",
    "    sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of what an image with additive noise looks like. The general image and - most importantly - the shape of the pattern on the turtle's head are preserved, while the original image is altered in such a way that the neural network does not get to see the same image another time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "example = ds.take(1).map(lambda x,y: (x + tf.random.normal(x.shape, mean=0.0, stddev=0.2, dtype=tf.float32), y))\n",
    "example = example.map(lambda x,y: (tf.clip_by_value(x, 0.0, 1.0), y))\n",
    "\n",
    "for elem in example.take(1):\n",
    "    im, label = elem\n",
    "plt.imshow(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.preprocessing.image.random_channel_shift\n",
    "#tf.keras.preprocessing.image.random_brightness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientNet = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(224,224,3)),\n",
    "    hub.KerasLayer(\"https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1\", trainable=False),\n",
    "    tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "efficientNet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientNet.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=[tf.keras.metrics.CategoricalAccuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientNet.fit(train_ds, epochs=10, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientNet.evaluate(test_ds)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "04b1a3d1b468328a21edc8325c3a847f7512f5ee526b7b9ad818cdbcda89e4b9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('turtleRecall')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
